{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla_GraphSAINT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeamykim/GCN-related-works/blob/main/vanilla_GraphSAINT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klL2ZtmK3mgG"
      },
      "source": [
        "References: \n",
        "\n",
        "Fast Graph Representation Learning with PyTorch Geometric\n",
        "https://arxiv.org/abs/1903.02428\n",
        "\n",
        "\n",
        "PyTorch Geometric Github https://github.com/rusty1s/pytorch_geometric/blob/master/examples/graph_saint.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-G-VhBuqDuY"
      },
      "source": [
        "#Install required packages, libraries, and datasets of PyTorch Geometric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqbdjYYGNTTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2da9fc-a08d-428d-b055-e9b9c6ff0ce8"
      },
      "source": [
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.9MB 54.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.3MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 18.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfTBmvnqTLC"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader, GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTSampler\n",
        "from torch_geometric.nn import GCNConv\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGrKO5YH11-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ebc9a6-3052-46a6-c5e2-ae535a64c4f9"
      },
      "source": [
        "from torch_geometric.datasets import Yelp, Amazon, Reddit\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Yelp(root='data/Yelp', transform=NormalizeFeatures()) # multi-class classification wtih 100 classes\n",
        "#dataset_amazon = Amazon(root='data/Amazon', name='computers') # multi- class classification\n",
        "#dataset_reddit = Reddit(root='data/Reddit',transform=NormalizeFeatures()) # single class classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1Juwx8HtDwSzmVIJ31ooVa1WljI4U5JnA into data/Yelp/raw/adj_full.npz... Done.\n",
            "Downloading 1Zy6BZH_zLEjKlEFSduKE5tV9qqA_8VtM into data/Yelp/raw/feats.npy... Done.\n",
            "Downloading 1VUcBGr0T0-klqerjAjxRmAqFuld_SMWU into data/Yelp/raw/class_map.json... Done.\n",
            "Downloading 1NI5pa5Chpd-52eSmLW60OnB3WS5ikxq_ into data/Yelp/raw/role.json... Done.\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Ljy30NqgS9"
      },
      "source": [
        "#Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjOnyLh9p9y5"
      },
      "source": [
        "Print basic information about the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It-8StB97H10"
      },
      "source": [
        "def print_data(input_data):\n",
        "  ''' input_data: pytorch geometric dataset format\n",
        "      prints basic information about the dataset\n",
        "  '''\n",
        "\n",
        "  print()\n",
        "  print(f'Dataset: {input_data}:')\n",
        "  print('======================')\n",
        "  print(f'Number of graphs: {len(input_data)}')\n",
        "  print(f'Number of features: {input_data.num_features}')\n",
        "  print(f'Number of classes: {input_data.num_classes}')\n",
        "\n",
        "  data = input_data[0]  # Get the first graph object.\n",
        "\n",
        "  print()\n",
        "  print(data)\n",
        "  print('===========================================================================================================')\n",
        "\n",
        "  # Gather some statistics about the graph.\n",
        "  print(f'Number of nodes: {data.num_nodes}')\n",
        "  print(f'Number of edges: {data.num_edges}')\n",
        "  print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "  #if data.train_mask: # to check if the data is masked with train/val/test mask\n",
        "  #  print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "  #  print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "  print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "  print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "  print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1MJ7Br27_6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398a75b1-b8c7-4af8-a2be-3f03560416f5"
      },
      "source": [
        "print_data(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: Yelp():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 300\n",
            "Number of classes: 100\n",
            "\n",
            "Data(edge_index=[2, 13954819], test_mask=[716847], train_mask=[716847], val_mask=[716847], x=[716847, 300], y=[716847, 100])\n",
            "===========================================================================================================\n",
            "Number of nodes: 716847\n",
            "Number of edges: 13954819\n",
            "Average node degree: 19.47\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: True\n",
            "Is undirected: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvUPHAKJqlpr"
      },
      "source": [
        "Define data and inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS54IJNQ9EV1"
      },
      "source": [
        "data = dataset[0] # the first graph object of the dataset (which has just 1 grph anyways)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSpyICpEwS_W",
        "outputId": "c8916f99-8f80-4d66-d03a-6d5b67f45168"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 13954819], test_mask=[716847], train_mask=[716847], val_mask=[716847], x=[716847, 300], y=[716847, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXR1rU5wp1F0"
      },
      "source": [
        "Some extra functions in case we cant to get information about the graph data (e.g. degree of the dataset). \n",
        "\n",
        "Reference: (insert link)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImJ-AfihkqV6"
      },
      "source": [
        "from typing import Optional\n",
        "\n",
        "def maybe_num_nodes(index: torch.Tensor,\n",
        "                    num_nodes: Optional[int] = None) -> int:\n",
        "    return int(index.max()) + 1 if num_nodes is None else num_nodes\n",
        "\n",
        "def degree(index, num_nodes: Optional[int] = None,\n",
        "           dtype: Optional[int] = None):\n",
        "    r\"\"\"Computes the (unweighted) degree of a given one-dimensional index\n",
        "    tensor.\n",
        "    Args:\n",
        "        index (LongTensor): Index tensor.\n",
        "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
        "            :obj:`max_val + 1` of :attr:`index`. (default: :obj:`None`)\n",
        "        dtype (:obj:`torch.dtype`, optional): The desired data type of the\n",
        "            returned tensor.\n",
        "    :rtype: :class:`Tensor`\n",
        "    \"\"\"\n",
        "    N = maybe_num_nodes(index, num_nodes)\n",
        "    out = torch.zeros((N, ), dtype=dtype, device=index.device)\n",
        "    one = torch.ones((index.size(0), ), dtype=out.dtype, device=out.device)\n",
        "    return out.scatter_add_(0, index, one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJo42mboklMW"
      },
      "source": [
        "row, col = data.edge_index\n",
        "data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--use_normalization', action='store_true')\n",
        "#args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSIV8_z8q7uf"
      },
      "source": [
        "Define data loader (e.g. GraphSAINTRandomWalk, GraphSAINTRandomNode, GraphSAINTRandomEdge)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6FJCpbI54dS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ffc270-933a-4d4a-eb12-c4ea5682dee5"
      },
      "source": [
        "loader = GraphSAINTRandomWalkSampler(data, batch_size=512, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=10,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute GraphSAINT normalization: : 7171207it [05:39, 21152.42it/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-JyaDwhrH6t"
      },
      "source": [
        "Another quick data inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygn8F40KNUO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec11e4b1-51da-4be9-ab87-af22f91b4d06"
      },
      "source": [
        "for data in loader:\n",
        "  print(data)\n",
        "  print(data.y)\n",
        "  print(data.y.shape)\n",
        "  print(torch.argmax(data.y))\n",
        "  print(torch.argmax(data.y, dim=-1))\n",
        "  print(data.y.argmax(dim=1))\n",
        "  print(data.train_mask)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 5163], edge_norm=[5163], node_norm=[1325], test_mask=[1325], train_mask=[1325], val_mask=[1325], x=[1325, 300], y=[1325, 100])\n",
            "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
            "torch.Size([1325, 100])\n",
            "tensor(1)\n",
            "tensor([1, 1, 3,  ..., 1, 1, 2])\n",
            "tensor([1, 1, 3,  ..., 1, 1, 2])\n",
            "tensor([True, True, True,  ..., True, True, True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytw8mIsTrK4j"
      },
      "source": [
        "#Define a model structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDUMnWU1rRWy"
      },
      "source": [
        "GraphSAINT: 2 layers of GCN with ReLU activation and LogSoftmax layer as output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsxLwlb3-aPN"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(Net, self).__init__()\n",
        "        in_channels = dataset.num_node_features\n",
        "        out_channels = dataset.num_classes\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        #self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        #self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
        "        self.lin = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
        "\n",
        "    def set_aggr(self, aggr):\n",
        "        self.conv1.aggr = aggr\n",
        "        self.conv2.aggr = aggr\n",
        "        #self.conv3.aggr = aggr\n",
        "\n",
        "    def forward(self, x0, edge_index, edge_weight=None):\n",
        "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
        "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
        "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
        "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
        "        #x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
        "        #x3 = F.dropout(x3, p=0.2, training=self.training)\n",
        "        #x = torch.cat([x1, x2, x3], dim=-1)\n",
        "        x = torch.cat([x1, x2], dim=-1)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "        #nn.Sigmoid()(preds) if self.sigmoid_loss else F.softmax(preds, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrDwAYv2rjCO"
      },
      "source": [
        "Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smyEfVmH-PO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eb4856-ecd3-4806-c187-b6389f532e21"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(hidden_channels=256).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): GCNConv(300, 256)\n",
            "  (conv2): GCNConv(256, 256)\n",
            "  (lin): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYf1BNXrx8N"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ZiTYcDr5dl"
      },
      "source": [
        "Train Function with negative log-likelihood loss from train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtwmlyZm-ccI"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    #model.set_aggr('mean')\n",
        "    #model.set_aggr('max')\n",
        "    model.set_aggr('add')\n",
        "    #model.set_aggr('none')\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #if args.use_normalization:\n",
        "        #    edge_weight = data.edge_norm * data.edge_weight\n",
        "        #    out = model(data.x, data.edge_index, edge_weight)\n",
        "        #    loss = F.nll_loss(out, data.y, reduction='none')\n",
        "        #    loss = (loss * data.node_norm)[data.train_mask].sum()\n",
        "        #else:\n",
        "        #    out = model(data.x, data.edge_index)\n",
        "        #    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "        out = model(data.x, data.edge_index)\n",
        "        #print(out.argmax(dim=-1))\n",
        "        loss = F.nll_loss(out[data.train_mask], torch.argmax(data.y[data.train_mask], dim=-1)) # for Yelp\n",
        "        #loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask]) # for Yelp\n",
        "        #loss = criterion(out[data.train_mask], torch.argmax(data.y[data.train_mask], dim=-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_nodes\n",
        "        total_examples += data.num_nodes\n",
        "\n",
        "    return total_loss / total_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hORrp2qIr_K2"
      },
      "source": [
        "Test the model with accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "959KTfdVaU8L"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFH2WLEZvzR7"
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    #model.set_aggr('mean')\n",
        "    #model.set_aggr('max')\n",
        "    model.set_aggr('add')\n",
        "    #model.set_aggr('none')\n",
        "\n",
        "    out = model(data.x.to(device), data.edge_index.to(device))\n",
        "    pred = out.argmax(dim=-1)\n",
        "    #correct = pred.eq(data.y.to(device))\n",
        "    correct = pred.eq(torch.argmax(data.y, dim=-1).to(device)) # for Yelp\n",
        "\n",
        "    accs = []\n",
        "    micro_scores = []\n",
        "    macro_scores = []\n",
        "\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(correct[mask].sum().item() / mask.sum().item())\n",
        "        \n",
        "        micro_score = f1_score(torch.argmax(data.y[mask], dim=-1).cpu(), pred[mask].cpu(), average='micro')\n",
        "        micro_scores.append(micro_score) \n",
        "\n",
        "        macro_score = f1_score(torch.argmax(data.y[mask], dim=-1).cpu(), pred[mask].cpu(), average='macro')\n",
        "        macro_scores.append(macro_score)\n",
        "\n",
        "\n",
        "    return accs, micro_scores, macro_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-EnAxmMsEO6"
      },
      "source": [
        "Print out the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OCgKvij51na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffd97ae-381c-4847-df31-06f6fbe71cde"
      },
      "source": [
        "for epoch in range(1, 51):\n",
        "    loss = train()\n",
        "    accs, micro_scores, macro_scores = test()\n",
        "\n",
        "    # Accuracy\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, '\n",
        "          f'Val: {accs[1]:.4f}, Test: {accs[2]:.4f}')\n",
        "    \n",
        "    #micro F1\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {micro_scores[0]:.4f}, '\n",
        "          f'Val: {micro_scores[1]:.4f}, Test: {micro_scores[2]:.4f}')\n",
        "    \n",
        "    #macro F1\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {macro_scores[0]:.4f}, '\n",
        "          f'Val: {macro_scores[1]:.4f}, Test: {macro_scores[2]:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Loss: 1.6986, Train: 0.5276, Val: 0.5217, Test: 0.5725\n",
            "Epoch: 01, Loss: 1.6986, Train: 0.5276, Val: 0.5217, Test: 0.5725\n",
            "Epoch: 01, Loss: 1.6986, Train: 0.2120, Val: 0.2369, Test: 0.3393\n",
            "Epoch: 02, Loss: 1.3529, Train: 0.5796, Val: 0.5362, Test: 0.6087\n",
            "Epoch: 02, Loss: 1.3529, Train: 0.5796, Val: 0.5362, Test: 0.6087\n",
            "Epoch: 02, Loss: 1.3529, Train: 0.2050, Val: 0.1368, Test: 0.4061\n",
            "Epoch: 03, Loss: 1.2754, Train: 0.5888, Val: 0.5604, Test: 0.6014\n",
            "Epoch: 03, Loss: 1.2754, Train: 0.5888, Val: 0.5604, Test: 0.6014\n",
            "Epoch: 03, Loss: 1.2754, Train: 0.2146, Val: 0.1959, Test: 0.4211\n",
            "Epoch: 04, Loss: 1.2460, Train: 0.5816, Val: 0.5362, Test: 0.5725\n",
            "Epoch: 04, Loss: 1.2460, Train: 0.5816, Val: 0.5362, Test: 0.5725\n",
            "Epoch: 04, Loss: 1.2460, Train: 0.2056, Val: 0.1565, Test: 0.3690\n",
            "Epoch: 05, Loss: 1.1929, Train: 0.5816, Val: 0.5459, Test: 0.5942\n",
            "Epoch: 05, Loss: 1.1929, Train: 0.5816, Val: 0.5459, Test: 0.5942\n",
            "Epoch: 05, Loss: 1.1929, Train: 0.1819, Val: 0.1821, Test: 0.3958\n",
            "Epoch: 06, Loss: 1.1917, Train: 0.5867, Val: 0.5749, Test: 0.6304\n",
            "Epoch: 06, Loss: 1.1917, Train: 0.5867, Val: 0.5749, Test: 0.6304\n",
            "Epoch: 06, Loss: 1.1917, Train: 0.1903, Val: 0.1820, Test: 0.3696\n",
            "Epoch: 07, Loss: 1.1809, Train: 0.5867, Val: 0.5749, Test: 0.6449\n",
            "Epoch: 07, Loss: 1.1809, Train: 0.5867, Val: 0.5749, Test: 0.6449\n",
            "Epoch: 07, Loss: 1.1809, Train: 0.1949, Val: 0.1739, Test: 0.4287\n",
            "Epoch: 08, Loss: 1.1920, Train: 0.5837, Val: 0.5507, Test: 0.5725\n",
            "Epoch: 08, Loss: 1.1920, Train: 0.5837, Val: 0.5507, Test: 0.5725\n",
            "Epoch: 08, Loss: 1.1920, Train: 0.1988, Val: 0.1474, Test: 0.3865\n",
            "Epoch: 09, Loss: 1.1676, Train: 0.5980, Val: 0.5749, Test: 0.5942\n",
            "Epoch: 09, Loss: 1.1676, Train: 0.5980, Val: 0.5749, Test: 0.5942\n",
            "Epoch: 09, Loss: 1.1676, Train: 0.2170, Val: 0.1837, Test: 0.4256\n",
            "Epoch: 10, Loss: 1.1732, Train: 0.5898, Val: 0.5700, Test: 0.5942\n",
            "Epoch: 10, Loss: 1.1732, Train: 0.5898, Val: 0.5700, Test: 0.5942\n",
            "Epoch: 10, Loss: 1.1732, Train: 0.2056, Val: 0.1723, Test: 0.4017\n",
            "Epoch: 11, Loss: 1.1892, Train: 0.5980, Val: 0.5652, Test: 0.6304\n",
            "Epoch: 11, Loss: 1.1892, Train: 0.5980, Val: 0.5652, Test: 0.6304\n",
            "Epoch: 11, Loss: 1.1892, Train: 0.2169, Val: 0.1788, Test: 0.3315\n",
            "Epoch: 12, Loss: 1.1402, Train: 0.5918, Val: 0.5700, Test: 0.6304\n",
            "Epoch: 12, Loss: 1.1402, Train: 0.5918, Val: 0.5700, Test: 0.6304\n",
            "Epoch: 12, Loss: 1.1402, Train: 0.2105, Val: 0.1770, Test: 0.3278\n",
            "Epoch: 13, Loss: 1.1864, Train: 0.5980, Val: 0.5700, Test: 0.5870\n",
            "Epoch: 13, Loss: 1.1864, Train: 0.5980, Val: 0.5700, Test: 0.5870\n",
            "Epoch: 13, Loss: 1.1864, Train: 0.1963, Val: 0.1637, Test: 0.3967\n",
            "Epoch: 14, Loss: 1.1645, Train: 0.5878, Val: 0.5314, Test: 0.5797\n",
            "Epoch: 14, Loss: 1.1645, Train: 0.5878, Val: 0.5314, Test: 0.5797\n",
            "Epoch: 14, Loss: 1.1645, Train: 0.2047, Val: 0.1458, Test: 0.3797\n",
            "Epoch: 15, Loss: 1.1795, Train: 0.5980, Val: 0.5749, Test: 0.6232\n",
            "Epoch: 15, Loss: 1.1795, Train: 0.5980, Val: 0.5749, Test: 0.6232\n",
            "Epoch: 15, Loss: 1.1795, Train: 0.2167, Val: 0.1947, Test: 0.3449\n",
            "Epoch: 16, Loss: 1.1748, Train: 0.6000, Val: 0.5556, Test: 0.6232\n",
            "Epoch: 16, Loss: 1.1748, Train: 0.6000, Val: 0.5556, Test: 0.6232\n",
            "Epoch: 16, Loss: 1.1748, Train: 0.2065, Val: 0.1862, Test: 0.3514\n",
            "Epoch: 17, Loss: 1.1887, Train: 0.6031, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 17, Loss: 1.1887, Train: 0.6031, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 17, Loss: 1.1887, Train: 0.2131, Val: 0.1709, Test: 0.3309\n",
            "Epoch: 18, Loss: 1.1628, Train: 0.5918, Val: 0.5990, Test: 0.5942\n",
            "Epoch: 18, Loss: 1.1628, Train: 0.5918, Val: 0.5990, Test: 0.5942\n",
            "Epoch: 18, Loss: 1.1628, Train: 0.1918, Val: 0.1970, Test: 0.3393\n",
            "Epoch: 19, Loss: 1.1463, Train: 0.5939, Val: 0.5845, Test: 0.6449\n",
            "Epoch: 19, Loss: 1.1463, Train: 0.5939, Val: 0.5845, Test: 0.6449\n",
            "Epoch: 19, Loss: 1.1463, Train: 0.2013, Val: 0.1798, Test: 0.3644\n",
            "Epoch: 20, Loss: 1.1876, Train: 0.5847, Val: 0.5797, Test: 0.6087\n",
            "Epoch: 20, Loss: 1.1876, Train: 0.5847, Val: 0.5797, Test: 0.6087\n",
            "Epoch: 20, Loss: 1.1876, Train: 0.1907, Val: 0.1973, Test: 0.3566\n",
            "Epoch: 21, Loss: 1.1540, Train: 0.5908, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 21, Loss: 1.1540, Train: 0.5908, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 21, Loss: 1.1540, Train: 0.2078, Val: 0.1992, Test: 0.2661\n",
            "Epoch: 22, Loss: 1.1596, Train: 0.6031, Val: 0.5700, Test: 0.5942\n",
            "Epoch: 22, Loss: 1.1596, Train: 0.6031, Val: 0.5700, Test: 0.5942\n",
            "Epoch: 22, Loss: 1.1596, Train: 0.2108, Val: 0.1787, Test: 0.2669\n",
            "Epoch: 23, Loss: 1.1480, Train: 0.6000, Val: 0.5845, Test: 0.6159\n",
            "Epoch: 23, Loss: 1.1480, Train: 0.6000, Val: 0.5845, Test: 0.6159\n",
            "Epoch: 23, Loss: 1.1480, Train: 0.2157, Val: 0.1686, Test: 0.3394\n",
            "Epoch: 24, Loss: 1.1697, Train: 0.5969, Val: 0.5652, Test: 0.5870\n",
            "Epoch: 24, Loss: 1.1697, Train: 0.5969, Val: 0.5652, Test: 0.5870\n",
            "Epoch: 24, Loss: 1.1697, Train: 0.2138, Val: 0.1671, Test: 0.3290\n",
            "Epoch: 25, Loss: 1.1840, Train: 0.5990, Val: 0.5749, Test: 0.6087\n",
            "Epoch: 25, Loss: 1.1840, Train: 0.5990, Val: 0.5749, Test: 0.6087\n",
            "Epoch: 25, Loss: 1.1840, Train: 0.2035, Val: 0.1688, Test: 0.3434\n",
            "Epoch: 26, Loss: 1.1496, Train: 0.5980, Val: 0.5894, Test: 0.5942\n",
            "Epoch: 26, Loss: 1.1496, Train: 0.5980, Val: 0.5894, Test: 0.5942\n",
            "Epoch: 26, Loss: 1.1496, Train: 0.2110, Val: 0.1748, Test: 0.3417\n",
            "Epoch: 27, Loss: 1.1804, Train: 0.5969, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 27, Loss: 1.1804, Train: 0.5969, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 27, Loss: 1.1804, Train: 0.2112, Val: 0.1774, Test: 0.3418\n",
            "Epoch: 28, Loss: 1.1515, Train: 0.6082, Val: 0.5797, Test: 0.6014\n",
            "Epoch: 28, Loss: 1.1515, Train: 0.6082, Val: 0.5797, Test: 0.6014\n",
            "Epoch: 28, Loss: 1.1515, Train: 0.2099, Val: 0.1924, Test: 0.3144\n",
            "Epoch: 29, Loss: 1.1690, Train: 0.5959, Val: 0.5797, Test: 0.5725\n",
            "Epoch: 29, Loss: 1.1690, Train: 0.5959, Val: 0.5797, Test: 0.5725\n",
            "Epoch: 29, Loss: 1.1690, Train: 0.1889, Val: 0.1670, Test: 0.2443\n",
            "Epoch: 30, Loss: 1.1731, Train: 0.5990, Val: 0.5942, Test: 0.6087\n",
            "Epoch: 30, Loss: 1.1731, Train: 0.5990, Val: 0.5942, Test: 0.6087\n",
            "Epoch: 30, Loss: 1.1731, Train: 0.2101, Val: 0.1810, Test: 0.3359\n",
            "Epoch: 31, Loss: 1.1389, Train: 0.5888, Val: 0.5700, Test: 0.5652\n",
            "Epoch: 31, Loss: 1.1389, Train: 0.5888, Val: 0.5700, Test: 0.5652\n",
            "Epoch: 31, Loss: 1.1389, Train: 0.1866, Val: 0.1855, Test: 0.2462\n",
            "Epoch: 32, Loss: 1.1574, Train: 0.6010, Val: 0.5797, Test: 0.5870\n",
            "Epoch: 32, Loss: 1.1574, Train: 0.6010, Val: 0.5797, Test: 0.5870\n",
            "Epoch: 32, Loss: 1.1574, Train: 0.2130, Val: 0.1776, Test: 0.2525\n",
            "Epoch: 33, Loss: 1.1717, Train: 0.5969, Val: 0.6039, Test: 0.5797\n",
            "Epoch: 33, Loss: 1.1717, Train: 0.5969, Val: 0.6039, Test: 0.5797\n",
            "Epoch: 33, Loss: 1.1717, Train: 0.2082, Val: 0.1888, Test: 0.3053\n",
            "Epoch: 34, Loss: 1.1266, Train: 0.6020, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 34, Loss: 1.1266, Train: 0.6020, Val: 0.5894, Test: 0.6014\n",
            "Epoch: 34, Loss: 1.1266, Train: 0.2091, Val: 0.1700, Test: 0.3275\n",
            "Epoch: 35, Loss: 1.1690, Train: 0.6082, Val: 0.5990, Test: 0.6159\n",
            "Epoch: 35, Loss: 1.1690, Train: 0.6082, Val: 0.5990, Test: 0.6159\n",
            "Epoch: 35, Loss: 1.1690, Train: 0.2176, Val: 0.1930, Test: 0.2982\n",
            "Epoch: 36, Loss: 1.1522, Train: 0.6112, Val: 0.6039, Test: 0.6232\n",
            "Epoch: 36, Loss: 1.1522, Train: 0.6112, Val: 0.6039, Test: 0.6232\n",
            "Epoch: 36, Loss: 1.1522, Train: 0.2046, Val: 0.2007, Test: 0.3249\n",
            "Epoch: 37, Loss: 1.1204, Train: 0.6092, Val: 0.5894, Test: 0.6087\n",
            "Epoch: 37, Loss: 1.1204, Train: 0.6092, Val: 0.5894, Test: 0.6087\n",
            "Epoch: 37, Loss: 1.1204, Train: 0.2140, Val: 0.1840, Test: 0.2774\n",
            "Epoch: 38, Loss: 1.1521, Train: 0.6010, Val: 0.5990, Test: 0.5942\n",
            "Epoch: 38, Loss: 1.1521, Train: 0.6010, Val: 0.5990, Test: 0.5942\n",
            "Epoch: 38, Loss: 1.1521, Train: 0.2078, Val: 0.1788, Test: 0.3106\n",
            "Epoch: 39, Loss: 1.1604, Train: 0.6010, Val: 0.5894, Test: 0.5870\n",
            "Epoch: 39, Loss: 1.1604, Train: 0.6010, Val: 0.5894, Test: 0.5870\n",
            "Epoch: 39, Loss: 1.1604, Train: 0.2108, Val: 0.1823, Test: 0.3209\n",
            "Epoch: 40, Loss: 1.1246, Train: 0.6020, Val: 0.5604, Test: 0.5870\n",
            "Epoch: 40, Loss: 1.1246, Train: 0.6020, Val: 0.5604, Test: 0.5870\n",
            "Epoch: 40, Loss: 1.1246, Train: 0.2064, Val: 0.1617, Test: 0.2997\n",
            "Epoch: 41, Loss: 1.1540, Train: 0.5939, Val: 0.5652, Test: 0.5725\n",
            "Epoch: 41, Loss: 1.1540, Train: 0.5939, Val: 0.5652, Test: 0.5725\n",
            "Epoch: 41, Loss: 1.1540, Train: 0.1878, Val: 0.1767, Test: 0.2526\n",
            "Epoch: 42, Loss: 1.1849, Train: 0.5969, Val: 0.5845, Test: 0.5870\n",
            "Epoch: 42, Loss: 1.1849, Train: 0.5969, Val: 0.5845, Test: 0.5870\n",
            "Epoch: 42, Loss: 1.1849, Train: 0.2077, Val: 0.1895, Test: 0.2284\n",
            "Epoch: 43, Loss: 1.1767, Train: 0.6020, Val: 0.5700, Test: 0.6087\n",
            "Epoch: 43, Loss: 1.1767, Train: 0.6020, Val: 0.5700, Test: 0.6087\n",
            "Epoch: 43, Loss: 1.1767, Train: 0.2245, Val: 0.1651, Test: 0.3072\n",
            "Epoch: 44, Loss: 1.1486, Train: 0.6102, Val: 0.5894, Test: 0.6087\n",
            "Epoch: 44, Loss: 1.1486, Train: 0.6102, Val: 0.5894, Test: 0.6087\n",
            "Epoch: 44, Loss: 1.1486, Train: 0.2074, Val: 0.1811, Test: 0.3532\n",
            "Epoch: 45, Loss: 1.1453, Train: 0.6041, Val: 0.5845, Test: 0.5870\n",
            "Epoch: 45, Loss: 1.1453, Train: 0.6041, Val: 0.5845, Test: 0.5870\n",
            "Epoch: 45, Loss: 1.1453, Train: 0.2154, Val: 0.1787, Test: 0.2946\n",
            "Epoch: 46, Loss: 1.1435, Train: 0.6051, Val: 0.5556, Test: 0.6159\n",
            "Epoch: 46, Loss: 1.1435, Train: 0.6051, Val: 0.5556, Test: 0.6159\n",
            "Epoch: 46, Loss: 1.1435, Train: 0.2150, Val: 0.1713, Test: 0.3161\n",
            "Epoch: 47, Loss: 1.1490, Train: 0.5939, Val: 0.5700, Test: 0.6014\n",
            "Epoch: 47, Loss: 1.1490, Train: 0.5939, Val: 0.5700, Test: 0.6014\n",
            "Epoch: 47, Loss: 1.1490, Train: 0.2111, Val: 0.1844, Test: 0.3393\n",
            "Epoch: 48, Loss: 1.1429, Train: 0.5959, Val: 0.5797, Test: 0.5797\n",
            "Epoch: 48, Loss: 1.1429, Train: 0.5959, Val: 0.5797, Test: 0.5797\n",
            "Epoch: 48, Loss: 1.1429, Train: 0.1938, Val: 0.1831, Test: 0.2551\n",
            "Epoch: 49, Loss: 1.1716, Train: 0.5980, Val: 0.5797, Test: 0.5652\n",
            "Epoch: 49, Loss: 1.1716, Train: 0.5980, Val: 0.5797, Test: 0.5652\n",
            "Epoch: 49, Loss: 1.1716, Train: 0.2028, Val: 0.1882, Test: 0.3004\n",
            "Epoch: 50, Loss: 1.1701, Train: 0.6000, Val: 0.5749, Test: 0.5580\n",
            "Epoch: 50, Loss: 1.1701, Train: 0.6000, Val: 0.5749, Test: 0.5580\n",
            "Epoch: 50, Loss: 1.1701, Train: 0.2188, Val: 0.1791, Test: 0.2886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBBCeLlAL0oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2c3b385-ed56-4516-a3d7-5d0726021eda"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zOh6IIeI3Op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56f61d61-8047-4a0e-eab8-0afb7817108b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIHo9Js1sGq8"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LohEgNXXsHba"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}